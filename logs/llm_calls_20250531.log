2025-05-31 01:58:52,698 - INFO - Starting LLM call, use_cache=True
2025-05-31 01:58:52,698 - INFO - Creating OpenAI client
2025-05-31 01:58:52,716 - INFO - Making API call to OpenAI
2025-05-31 01:59:27,669 - INFO - Successfully received response from OpenAI
2025-05-31 01:59:37,673 - INFO - Starting LLM call, use_cache=False
2025-05-31 01:59:37,674 - INFO - Creating OpenAI client
2025-05-31 01:59:37,684 - INFO - Making API call to OpenAI
2025-05-31 02:00:07,962 - INFO - Successfully received response from OpenAI
2025-05-31 02:00:17,968 - INFO - Starting LLM call, use_cache=False
2025-05-31 02:00:17,968 - INFO - Creating OpenAI client
2025-05-31 02:00:17,978 - INFO - Making API call to OpenAI
2025-05-31 02:00:33,276 - INFO - Successfully received response from OpenAI
2025-05-31 02:03:57,298 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:03:57,298 - INFO - Creating OpenAI client
2025-05-31 02:03:57,310 - INFO - Making API call to OpenAI
2025-05-31 02:04:11,681 - INFO - Successfully received response from OpenAI
2025-05-31 02:06:59,216 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:06:59,216 - INFO - Creating OpenAI client
2025-05-31 02:06:59,228 - INFO - Making API call to OpenAI
2025-05-31 02:07:12,840 - INFO - Successfully received response from OpenAI
2025-05-31 02:07:12,841 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:07:12,841 - INFO - Creating OpenAI client
2025-05-31 02:07:12,844 - INFO - Making API call to OpenAI
2025-05-31 02:07:43,342 - INFO - Successfully received response from OpenAI
2025-05-31 02:12:40,307 - ERROR - Error in call_llm: No module named 'openai'
2025-05-31 02:12:40,308 - ERROR - Exception type: ModuleNotFoundError
2025-05-31 02:12:40,308 - ERROR - Traceback: Traceback (most recent call last):
  File "/Users/roshinpv/Documents/Projects/migraite/utils/call_llm.py", line 112, in call_llm
    from openai import OpenAI
ModuleNotFoundError: No module named 'openai'

2025-05-31 02:12:40,308 - INFO - Returning fallback response: I'm sorry, but I encountered an error while processing your request. Please try again later.
2025-05-31 02:12:50,314 - ERROR - Error in call_llm: No module named 'openai'
2025-05-31 02:12:50,315 - ERROR - Exception type: ModuleNotFoundError
2025-05-31 02:12:50,315 - ERROR - Traceback: Traceback (most recent call last):
  File "/Users/roshinpv/Documents/Projects/migraite/utils/call_llm.py", line 112, in call_llm
    from openai import OpenAI
ModuleNotFoundError: No module named 'openai'

2025-05-31 02:12:50,315 - INFO - Returning fallback response: I'm sorry, but I encountered an error while processing your request. Please try again later.
2025-05-31 02:13:00,322 - ERROR - Error in call_llm: No module named 'openai'
2025-05-31 02:13:00,322 - ERROR - Exception type: ModuleNotFoundError
2025-05-31 02:13:00,322 - ERROR - Traceback: Traceback (most recent call last):
  File "/Users/roshinpv/Documents/Projects/migraite/utils/call_llm.py", line 112, in call_llm
    from openai import OpenAI
ModuleNotFoundError: No module named 'openai'

2025-05-31 02:13:00,323 - INFO - Returning fallback response: I'm sorry, but I encountered an error while processing your request. Please try again later.
2025-05-31 02:13:58,381 - ERROR - Error in call_llm: No module named 'openai'
2025-05-31 02:13:58,382 - ERROR - Exception type: ModuleNotFoundError
2025-05-31 02:13:58,382 - ERROR - Traceback: Traceback (most recent call last):
  File "/Users/roshinpv/Documents/Projects/migraite/utils/call_llm.py", line 112, in call_llm
    from openai import OpenAI
ModuleNotFoundError: No module named 'openai'

2025-05-31 02:13:58,382 - INFO - Returning fallback response: I'm sorry, but I encountered an error while processing your request. Please try again later.
2025-05-31 02:14:08,385 - ERROR - Error in call_llm: No module named 'openai'
2025-05-31 02:14:08,386 - ERROR - Exception type: ModuleNotFoundError
2025-05-31 02:14:08,386 - ERROR - Traceback: Traceback (most recent call last):
  File "/Users/roshinpv/Documents/Projects/migraite/utils/call_llm.py", line 112, in call_llm
    from openai import OpenAI
ModuleNotFoundError: No module named 'openai'

2025-05-31 02:14:08,386 - INFO - Returning fallback response: I'm sorry, but I encountered an error while processing your request. Please try again later.
2025-05-31 02:14:29,529 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:14:29,529 - WARNING - Prompt too large: ~14831 tokens. Truncating to ~8000 tokens.
2025-05-31 02:14:29,529 - INFO - Creating OpenAI client
2025-05-31 02:14:29,552 - INFO - Making API call to OpenAI
2025-05-31 02:15:06,054 - INFO - Successfully received response from OpenAI
2025-05-31 02:15:06,055 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:15:06,055 - INFO - Creating OpenAI client
2025-05-31 02:15:06,063 - INFO - Making API call to OpenAI
2025-05-31 02:15:30,619 - INFO - Successfully received response from OpenAI
2025-05-31 02:15:40,621 - INFO - Starting LLM call, use_cache=False
2025-05-31 02:15:40,621 - INFO - Creating OpenAI client
2025-05-31 02:15:40,634 - INFO - Making API call to OpenAI
2025-05-31 02:16:33,254 - INFO - Successfully received response from OpenAI
2025-05-31 02:21:47,319 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:21:47,319 - INFO - Creating OpenAI client
2025-05-31 02:21:47,337 - INFO - Making API call to OpenAI
2025-05-31 02:22:15,838 - INFO - Successfully received response from OpenAI
2025-05-31 02:22:15,839 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:22:15,839 - INFO - Creating OpenAI client
2025-05-31 02:22:15,843 - INFO - Making API call to OpenAI
2025-05-31 02:22:48,090 - INFO - Successfully received response from OpenAI
2025-05-31 02:22:58,097 - INFO - Starting LLM call, use_cache=False
2025-05-31 02:22:58,097 - INFO - Creating OpenAI client
2025-05-31 02:22:58,105 - INFO - Making API call to OpenAI
2025-05-31 02:23:38,399 - INFO - Successfully received response from OpenAI
2025-05-31 02:23:48,401 - INFO - Starting LLM call, use_cache=False
2025-05-31 02:23:48,401 - INFO - Creating OpenAI client
2025-05-31 02:23:48,409 - INFO - Making API call to OpenAI
2025-05-31 02:24:33,506 - INFO - Successfully received response from OpenAI
2025-05-31 02:34:40,170 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:34:40,170 - INFO - Creating OpenAI client
2025-05-31 02:34:40,189 - INFO - Making API call to OpenAI
2025-05-31 02:34:45,491 - INFO - Successfully received response from OpenAI
2025-05-31 02:34:45,494 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:34:45,494 - INFO - Creating OpenAI client
2025-05-31 02:34:45,498 - INFO - Making API call to OpenAI
2025-05-31 02:34:49,849 - INFO - Successfully received response from OpenAI
2025-05-31 02:34:49,854 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:34:49,854 - INFO - Creating OpenAI client
2025-05-31 02:34:49,863 - INFO - Making API call to OpenAI
2025-05-31 02:34:51,078 - INFO - Successfully received response from OpenAI
2025-05-31 02:34:51,080 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:34:51,080 - INFO - Creating OpenAI client
2025-05-31 02:34:51,086 - INFO - Making API call to OpenAI
2025-05-31 02:35:04,175 - INFO - Successfully received response from OpenAI
2025-05-31 02:35:04,176 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:35:04,176 - INFO - Creating OpenAI client
2025-05-31 02:35:04,183 - INFO - Making API call to OpenAI
2025-05-31 02:35:20,065 - INFO - Successfully received response from OpenAI
2025-05-31 02:35:20,065 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:35:20,065 - INFO - Creating OpenAI client
2025-05-31 02:35:20,073 - INFO - Making API call to OpenAI
2025-05-31 02:35:41,296 - INFO - Successfully received response from OpenAI
2025-05-31 02:35:41,296 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:35:41,296 - INFO - Creating OpenAI client
2025-05-31 02:35:41,303 - INFO - Making API call to OpenAI
2025-05-31 02:36:00,920 - INFO - Successfully received response from OpenAI
2025-05-31 02:36:00,920 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:36:00,920 - INFO - Creating OpenAI client
2025-05-31 02:36:00,924 - INFO - Making API call to OpenAI
2025-05-31 02:36:16,154 - INFO - Successfully received response from OpenAI
2025-05-31 02:36:16,154 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:36:16,154 - INFO - Creating OpenAI client
2025-05-31 02:36:16,158 - INFO - Making API call to OpenAI
2025-05-31 02:36:33,852 - INFO - Successfully received response from OpenAI
2025-05-31 02:36:33,852 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:36:33,852 - WARNING - Prompt too large: ~8084 tokens. Truncating to ~8000 tokens.
2025-05-31 02:36:33,852 - INFO - Creating OpenAI client
2025-05-31 02:36:33,860 - INFO - Making API call to OpenAI
2025-05-31 02:36:56,580 - INFO - Successfully received response from OpenAI
2025-05-31 02:36:56,580 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:36:56,580 - WARNING - Prompt too large: ~9188 tokens. Truncating to ~8000 tokens.
2025-05-31 02:36:56,580 - INFO - Creating OpenAI client
2025-05-31 02:36:56,585 - INFO - Making API call to OpenAI
2025-05-31 02:37:22,370 - INFO - Successfully received response from OpenAI
2025-05-31 02:37:22,370 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:37:22,370 - WARNING - Prompt too large: ~10333 tokens. Truncating to ~8000 tokens.
2025-05-31 02:37:22,370 - INFO - Creating OpenAI client
2025-05-31 02:37:22,378 - INFO - Making API call to OpenAI
2025-05-31 02:43:46,219 - INFO - Starting LLM call, use_cache=True
2025-05-31 02:43:46,219 - INFO - Creating OpenAI client
2025-05-31 02:43:46,236 - INFO - Making API call to OpenAI
2025-05-31 03:00:00,744 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:00:00,744 - WARNING - Prompt too large: ~15086 tokens. Truncating to ~8000 tokens.
2025-05-31 03:00:00,744 - INFO - Creating OpenAI client
2025-05-31 03:00:00,762 - INFO - Making API call to OpenAI
2025-05-31 03:00:36,933 - INFO - Successfully received response from OpenAI
2025-05-31 03:00:46,938 - INFO - Starting LLM call, use_cache=False
2025-05-31 03:00:46,939 - WARNING - Prompt too large: ~15086 tokens. Truncating to ~8000 tokens.
2025-05-31 03:00:46,939 - INFO - Creating OpenAI client
2025-05-31 03:00:46,953 - INFO - Making API call to OpenAI
2025-05-31 03:01:22,347 - INFO - Successfully received response from OpenAI
2025-05-31 03:01:32,350 - INFO - Starting LLM call, use_cache=False
2025-05-31 03:01:32,350 - WARNING - Prompt too large: ~15086 tokens. Truncating to ~8000 tokens.
2025-05-31 03:01:32,351 - INFO - Creating OpenAI client
2025-05-31 03:01:32,363 - INFO - Making API call to OpenAI
2025-05-31 03:02:21,523 - INFO - Successfully received response from OpenAI
2025-05-31 03:02:21,524 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:02:21,524 - INFO - Creating OpenAI client
2025-05-31 03:02:21,530 - INFO - Making API call to OpenAI
2025-05-31 03:02:57,550 - INFO - Successfully received response from OpenAI
2025-05-31 03:02:57,555 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:02:57,555 - INFO - Creating OpenAI client
2025-05-31 03:02:57,560 - INFO - Making API call to OpenAI
2025-05-31 03:03:13,073 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:13,073 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:13,073 - INFO - Creating OpenAI client
2025-05-31 03:03:13,078 - INFO - Making API call to OpenAI
2025-05-31 03:03:18,726 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:18,726 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:18,726 - INFO - Creating OpenAI client
2025-05-31 03:03:18,735 - INFO - Making API call to OpenAI
2025-05-31 03:03:22,243 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:22,244 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:22,244 - INFO - Creating OpenAI client
2025-05-31 03:03:22,248 - INFO - Making API call to OpenAI
2025-05-31 03:03:24,631 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:24,631 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:24,631 - INFO - Creating OpenAI client
2025-05-31 03:03:24,640 - INFO - Making API call to OpenAI
2025-05-31 03:03:28,009 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:28,009 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:28,009 - INFO - Creating OpenAI client
2025-05-31 03:03:28,016 - INFO - Making API call to OpenAI
2025-05-31 03:03:30,857 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:30,857 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:30,857 - INFO - Creating OpenAI client
2025-05-31 03:03:30,867 - INFO - Making API call to OpenAI
2025-05-31 03:03:34,624 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:34,624 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:34,624 - INFO - Creating OpenAI client
2025-05-31 03:03:34,635 - INFO - Making API call to OpenAI
2025-05-31 03:03:42,178 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:42,178 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:42,178 - INFO - Creating OpenAI client
2025-05-31 03:03:42,186 - INFO - Making API call to OpenAI
2025-05-31 03:03:48,995 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:48,995 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:48,995 - INFO - Creating OpenAI client
2025-05-31 03:03:49,003 - INFO - Making API call to OpenAI
2025-05-31 03:03:52,784 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:52,784 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:52,784 - INFO - Creating OpenAI client
2025-05-31 03:03:52,792 - INFO - Making API call to OpenAI
2025-05-31 03:03:55,907 - INFO - Successfully received response from OpenAI
2025-05-31 03:03:55,908 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:03:55,908 - INFO - Creating OpenAI client
2025-05-31 03:03:55,917 - INFO - Making API call to OpenAI
2025-05-31 03:04:01,451 - INFO - Successfully received response from OpenAI
2025-05-31 03:04:01,452 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:04:01,452 - INFO - Creating OpenAI client
2025-05-31 03:04:01,462 - INFO - Making API call to OpenAI
2025-05-31 03:04:08,883 - INFO - Successfully received response from OpenAI
2025-05-31 03:04:08,884 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:04:08,884 - INFO - Creating OpenAI client
2025-05-31 03:04:08,891 - INFO - Making API call to OpenAI
2025-05-31 03:04:13,534 - INFO - Successfully received response from OpenAI
2025-05-31 03:04:13,535 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:04:13,535 - INFO - Creating OpenAI client
2025-05-31 03:04:13,544 - INFO - Making API call to OpenAI
2025-05-31 03:04:21,708 - INFO - Successfully received response from OpenAI
2025-05-31 03:04:21,708 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:04:21,708 - INFO - Creating OpenAI client
2025-05-31 03:04:21,713 - INFO - Making API call to OpenAI
2025-05-31 03:04:29,683 - INFO - Successfully received response from OpenAI
2025-05-31 03:04:29,683 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:04:29,683 - INFO - Creating OpenAI client
2025-05-31 03:04:29,692 - INFO - Making API call to OpenAI
2025-05-31 03:04:37,989 - INFO - Successfully received response from OpenAI
2025-05-31 03:04:37,989 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:04:37,989 - INFO - Creating OpenAI client
2025-05-31 03:04:37,997 - INFO - Making API call to OpenAI
2025-05-31 03:04:45,180 - INFO - Successfully received response from OpenAI
2025-05-31 03:04:45,180 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:04:45,180 - INFO - Creating OpenAI client
2025-05-31 03:04:45,189 - INFO - Making API call to OpenAI
2025-05-31 03:04:50,717 - INFO - Successfully received response from OpenAI
2025-05-31 03:04:50,717 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:04:50,718 - INFO - Creating OpenAI client
2025-05-31 03:04:50,728 - INFO - Making API call to OpenAI
2025-05-31 03:04:58,234 - INFO - Successfully received response from OpenAI
2025-05-31 03:04:58,234 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:04:58,234 - INFO - Creating OpenAI client
2025-05-31 03:04:58,238 - INFO - Making API call to OpenAI
2025-05-31 03:05:04,587 - INFO - Successfully received response from OpenAI
2025-05-31 03:05:04,587 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:05:04,587 - INFO - Creating OpenAI client
2025-05-31 03:05:04,597 - INFO - Making API call to OpenAI
2025-05-31 03:05:09,647 - INFO - Successfully received response from OpenAI
2025-05-31 03:05:09,648 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:05:09,648 - INFO - Creating OpenAI client
2025-05-31 03:05:09,658 - INFO - Making API call to OpenAI
2025-05-31 03:05:16,381 - INFO - Successfully received response from OpenAI
2025-05-31 03:05:16,381 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:05:16,381 - INFO - Creating OpenAI client
2025-05-31 03:05:16,391 - INFO - Making API call to OpenAI
2025-05-31 03:10:11,961 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:11,962 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:11,962 - INFO - Creating OpenAI client
2025-05-31 03:10:11,967 - INFO - Making API call to OpenAI
2025-05-31 03:10:16,419 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:16,419 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:16,420 - INFO - Creating OpenAI client
2025-05-31 03:10:16,429 - INFO - Making API call to OpenAI
2025-05-31 03:10:20,702 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:20,703 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:20,703 - INFO - Creating OpenAI client
2025-05-31 03:10:20,713 - INFO - Making API call to OpenAI
2025-05-31 03:10:24,583 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:24,584 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:24,584 - INFO - Creating OpenAI client
2025-05-31 03:10:24,595 - INFO - Making API call to OpenAI
2025-05-31 03:10:28,831 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:28,831 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:28,831 - INFO - Creating OpenAI client
2025-05-31 03:10:28,840 - INFO - Making API call to OpenAI
2025-05-31 03:10:34,227 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:34,227 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:34,228 - INFO - Creating OpenAI client
2025-05-31 03:10:34,237 - INFO - Making API call to OpenAI
2025-05-31 03:10:41,405 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:41,405 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:41,405 - INFO - Creating OpenAI client
2025-05-31 03:10:41,415 - INFO - Making API call to OpenAI
2025-05-31 03:10:48,091 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:48,092 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:48,092 - INFO - Creating OpenAI client
2025-05-31 03:10:48,101 - INFO - Making API call to OpenAI
2025-05-31 03:10:52,512 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:52,512 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:52,512 - INFO - Creating OpenAI client
2025-05-31 03:10:52,522 - INFO - Making API call to OpenAI
2025-05-31 03:10:57,983 - INFO - Successfully received response from OpenAI
2025-05-31 03:10:57,984 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:10:57,984 - INFO - Creating OpenAI client
2025-05-31 03:10:57,993 - INFO - Making API call to OpenAI
2025-05-31 03:11:02,209 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:02,210 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:02,210 - INFO - Creating OpenAI client
2025-05-31 03:11:02,220 - INFO - Making API call to OpenAI
2025-05-31 03:11:06,224 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:06,225 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:06,225 - INFO - Creating OpenAI client
2025-05-31 03:11:06,234 - INFO - Making API call to OpenAI
2025-05-31 03:11:10,807 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:10,807 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:10,807 - INFO - Creating OpenAI client
2025-05-31 03:11:10,817 - INFO - Making API call to OpenAI
2025-05-31 03:11:15,372 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:15,372 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:15,372 - INFO - Creating OpenAI client
2025-05-31 03:11:15,382 - INFO - Making API call to OpenAI
2025-05-31 03:11:23,565 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:23,565 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:23,565 - INFO - Creating OpenAI client
2025-05-31 03:11:23,572 - INFO - Making API call to OpenAI
2025-05-31 03:11:28,210 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:28,210 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:28,210 - INFO - Creating OpenAI client
2025-05-31 03:11:28,220 - INFO - Making API call to OpenAI
2025-05-31 03:11:35,007 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:35,007 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:35,007 - INFO - Creating OpenAI client
2025-05-31 03:11:35,017 - INFO - Making API call to OpenAI
2025-05-31 03:11:39,981 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:39,981 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:39,981 - INFO - Creating OpenAI client
2025-05-31 03:11:39,990 - INFO - Making API call to OpenAI
2025-05-31 03:11:44,967 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:44,967 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:44,967 - INFO - Creating OpenAI client
2025-05-31 03:11:44,977 - INFO - Making API call to OpenAI
2025-05-31 03:11:51,446 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:51,446 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:51,446 - INFO - Creating OpenAI client
2025-05-31 03:11:51,455 - INFO - Making API call to OpenAI
2025-05-31 03:11:59,842 - INFO - Successfully received response from OpenAI
2025-05-31 03:11:59,842 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:11:59,842 - INFO - Creating OpenAI client
2025-05-31 03:11:59,849 - INFO - Making API call to OpenAI
2025-05-31 03:12:06,661 - INFO - Successfully received response from OpenAI
2025-05-31 03:12:06,661 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:12:06,662 - INFO - Creating OpenAI client
2025-05-31 03:12:06,671 - INFO - Making API call to OpenAI
2025-05-31 03:12:13,378 - INFO - Successfully received response from OpenAI
2025-05-31 03:12:13,378 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:12:13,378 - INFO - Creating OpenAI client
2025-05-31 03:12:13,389 - INFO - Making API call to OpenAI
2025-05-31 03:12:21,572 - INFO - Successfully received response from OpenAI
2025-05-31 03:12:21,572 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:12:21,572 - INFO - Creating OpenAI client
2025-05-31 03:12:21,583 - INFO - Making API call to OpenAI
2025-05-31 03:12:32,885 - INFO - Successfully received response from OpenAI
2025-05-31 03:12:32,885 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:12:32,885 - INFO - Creating OpenAI client
2025-05-31 03:12:32,894 - INFO - Making API call to OpenAI
2025-05-31 03:12:37,984 - INFO - Successfully received response from OpenAI
2025-05-31 03:12:37,984 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:12:37,984 - INFO - Creating OpenAI client
2025-05-31 03:12:37,994 - INFO - Making API call to OpenAI
2025-05-31 03:12:45,935 - INFO - Successfully received response from OpenAI
2025-05-31 03:12:45,935 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:12:45,935 - INFO - Creating OpenAI client
2025-05-31 03:12:45,946 - INFO - Making API call to OpenAI
2025-05-31 03:12:58,396 - INFO - Successfully received response from OpenAI
2025-05-31 03:12:58,397 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:12:58,397 - INFO - Creating OpenAI client
2025-05-31 03:12:58,406 - INFO - Making API call to OpenAI
2025-05-31 03:13:03,206 - INFO - Successfully received response from OpenAI
2025-05-31 03:13:03,206 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:13:03,206 - INFO - Creating OpenAI client
2025-05-31 03:13:03,215 - INFO - Making API call to OpenAI
2025-05-31 03:13:11,036 - INFO - Successfully received response from OpenAI
2025-05-31 03:13:11,036 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:13:11,036 - INFO - Creating OpenAI client
2025-05-31 03:13:11,045 - INFO - Making API call to OpenAI
2025-05-31 03:13:15,883 - INFO - Successfully received response from OpenAI
2025-05-31 03:13:15,883 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:13:15,883 - INFO - Creating OpenAI client
2025-05-31 03:13:15,892 - INFO - Making API call to OpenAI
2025-05-31 03:13:19,419 - INFO - Successfully received response from OpenAI
2025-05-31 03:13:19,419 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:13:19,420 - INFO - Creating OpenAI client
2025-05-31 03:13:19,430 - INFO - Making API call to OpenAI
2025-05-31 03:13:24,463 - INFO - Successfully received response from OpenAI
2025-05-31 03:13:24,463 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:13:24,463 - INFO - Creating OpenAI client
2025-05-31 03:13:24,473 - INFO - Making API call to OpenAI
2025-05-31 03:13:29,068 - INFO - Successfully received response from OpenAI
2025-05-31 03:13:29,068 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:13:29,068 - INFO - Creating OpenAI client
2025-05-31 03:13:29,078 - INFO - Making API call to OpenAI
2025-05-31 03:13:37,075 - INFO - Successfully received response from OpenAI
2025-05-31 03:13:37,076 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:13:37,076 - INFO - Creating OpenAI client
2025-05-31 03:13:37,084 - INFO - Making API call to OpenAI
2025-05-31 03:13:41,826 - INFO - Successfully received response from OpenAI
2025-05-31 03:13:41,827 - INFO - Starting LLM call, use_cache=True
2025-05-31 03:13:41,827 - INFO - Creating OpenAI client
2025-05-31 03:13:41,833 - INFO - Making API call to OpenAI
2025-05-31 03:13:51,497 - INFO - Successfully received response from OpenAI
2025-05-31 10:01:20,084 - INFO - Starting LLM call, use_cache=True
2025-05-31 10:01:20,084 - INFO - Creating OpenAI client
2025-05-31 10:01:20,103 - INFO - Making API call to OpenAI
2025-05-31 10:01:36,334 - INFO - Successfully received response from OpenAI
2025-05-31 10:01:36,334 - INFO - Starting LLM call, use_cache=True
2025-05-31 10:01:36,334 - INFO - Creating OpenAI client
2025-05-31 10:01:36,339 - INFO - Making API call to OpenAI
2025-05-31 10:01:56,497 - INFO - Successfully received response from OpenAI
2025-05-31 12:49:09,857 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:49:09,857 - INFO - Creating OpenAI client
2025-05-31 12:49:09,875 - INFO - Making API call to OpenAI
2025-05-31 12:49:32,961 - INFO - Successfully received response from OpenAI
2025-05-31 12:50:44,139 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:50:44,139 - INFO - Creating OpenAI client
2025-05-31 12:50:44,151 - INFO - Making API call to OpenAI
2025-05-31 12:51:04,437 - INFO - Successfully received response from OpenAI
2025-05-31 12:51:04,438 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:51:04,438 - INFO - Creating OpenAI client
2025-05-31 12:51:04,441 - INFO - Making API call to OpenAI
2025-05-31 12:51:56,555 - INFO - Successfully received response from OpenAI
2025-05-31 12:51:56,555 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:51:56,555 - INFO - Creating OpenAI client
2025-05-31 12:51:56,561 - INFO - Making API call to OpenAI
2025-05-31 12:52:23,638 - INFO - Successfully received response from OpenAI
2025-05-31 12:52:23,638 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:52:23,638 - INFO - Creating OpenAI client
2025-05-31 12:52:23,646 - INFO - Making API call to OpenAI
2025-05-31 12:52:32,589 - INFO - Successfully received response from OpenAI
2025-05-31 12:52:32,590 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:52:32,590 - INFO - Creating OpenAI client
2025-05-31 12:52:32,596 - INFO - Making API call to OpenAI
2025-05-31 12:52:47,365 - INFO - Successfully received response from OpenAI
2025-05-31 12:52:47,368 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:52:47,368 - INFO - Creating OpenAI client
2025-05-31 12:52:47,374 - INFO - Making API call to OpenAI
2025-05-31 12:52:53,122 - INFO - Successfully received response from OpenAI
2025-05-31 12:52:53,122 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:52:53,122 - INFO - Creating OpenAI client
2025-05-31 12:52:53,129 - INFO - Making API call to OpenAI
2025-05-31 12:52:58,819 - INFO - Successfully received response from OpenAI
2025-05-31 12:52:58,820 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:52:58,820 - INFO - Creating OpenAI client
2025-05-31 12:52:58,833 - INFO - Making API call to OpenAI
2025-05-31 12:53:00,797 - INFO - Successfully received response from OpenAI
2025-05-31 12:53:00,797 - INFO - Starting LLM call, use_cache=True
2025-05-31 12:53:00,797 - INFO - Creating OpenAI client
2025-05-31 12:53:00,821 - INFO - Making API call to OpenAI
2025-05-31 12:53:07,823 - INFO - Successfully received response from OpenAI
2025-05-31 14:02:06,454 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:02:06,454 - WARNING - Prompt too large: ~15369 tokens. Truncating to ~8000 tokens.
2025-05-31 14:02:06,454 - INFO - Creating OpenAI client
2025-05-31 14:02:06,474 - INFO - Making API call to OpenAI
2025-05-31 14:02:37,392 - INFO - Successfully received response from OpenAI
2025-05-31 14:02:37,395 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:02:37,395 - INFO - Creating OpenAI client
2025-05-31 14:02:37,400 - INFO - Making API call to OpenAI
2025-05-31 14:02:54,488 - INFO - Successfully received response from OpenAI
2025-05-31 14:02:54,488 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:02:54,488 - INFO - Creating OpenAI client
2025-05-31 14:02:54,493 - INFO - Making API call to OpenAI
2025-05-31 14:03:02,874 - INFO - Successfully received response from OpenAI
2025-05-31 14:03:02,875 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:03:02,875 - INFO - Creating OpenAI client
2025-05-31 14:03:02,882 - INFO - Making API call to OpenAI
2025-05-31 14:03:38,588 - INFO - Successfully received response from OpenAI
2025-05-31 14:03:38,600 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:03:38,601 - INFO - Creating OpenAI client
2025-05-31 14:03:38,605 - INFO - Making API call to OpenAI
2025-05-31 14:13:40,653 - INFO - Successfully received response from OpenAI
2025-05-31 14:13:40,653 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:13:40,653 - INFO - Creating OpenAI client
2025-05-31 14:13:40,662 - INFO - Making API call to OpenAI
2025-05-31 14:14:06,606 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:06,606 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:06,606 - INFO - Creating OpenAI client
2025-05-31 14:14:06,613 - INFO - Making API call to OpenAI
2025-05-31 14:14:08,743 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:08,743 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:08,743 - INFO - Creating OpenAI client
2025-05-31 14:14:08,750 - INFO - Making API call to OpenAI
2025-05-31 14:14:11,025 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:11,029 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:11,030 - INFO - Creating OpenAI client
2025-05-31 14:14:11,038 - INFO - Making API call to OpenAI
2025-05-31 14:14:13,528 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:13,529 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:13,529 - INFO - Creating OpenAI client
2025-05-31 14:14:13,536 - INFO - Making API call to OpenAI
2025-05-31 14:14:15,994 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:15,994 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:15,994 - INFO - Creating OpenAI client
2025-05-31 14:14:16,002 - INFO - Making API call to OpenAI
2025-05-31 14:14:18,648 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:18,648 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:18,648 - INFO - Creating OpenAI client
2025-05-31 14:14:18,655 - INFO - Making API call to OpenAI
2025-05-31 14:14:19,904 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:19,904 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:19,904 - INFO - Creating OpenAI client
2025-05-31 14:14:19,911 - INFO - Making API call to OpenAI
2025-05-31 14:14:21,904 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:21,905 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:21,905 - INFO - Creating OpenAI client
2025-05-31 14:14:21,914 - INFO - Making API call to OpenAI
2025-05-31 14:14:24,278 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:24,278 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:24,278 - INFO - Creating OpenAI client
2025-05-31 14:14:24,285 - INFO - Making API call to OpenAI
2025-05-31 14:14:28,993 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:28,994 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:28,994 - INFO - Creating OpenAI client
2025-05-31 14:14:29,001 - INFO - Making API call to OpenAI
2025-05-31 14:14:31,466 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:31,466 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:31,467 - INFO - Creating OpenAI client
2025-05-31 14:14:31,488 - INFO - Making API call to OpenAI
2025-05-31 14:14:33,733 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:33,734 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:33,734 - INFO - Creating OpenAI client
2025-05-31 14:14:33,741 - INFO - Making API call to OpenAI
2025-05-31 14:14:38,482 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:38,482 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:38,482 - INFO - Creating OpenAI client
2025-05-31 14:14:38,491 - INFO - Making API call to OpenAI
2025-05-31 14:14:42,309 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:42,309 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:42,309 - INFO - Creating OpenAI client
2025-05-31 14:14:42,316 - INFO - Making API call to OpenAI
2025-05-31 14:14:44,696 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:44,696 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:44,696 - INFO - Creating OpenAI client
2025-05-31 14:14:44,700 - INFO - Making API call to OpenAI
2025-05-31 14:14:47,393 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:47,394 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:47,394 - INFO - Creating OpenAI client
2025-05-31 14:14:47,400 - INFO - Making API call to OpenAI
2025-05-31 14:14:49,950 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:49,951 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:49,951 - INFO - Creating OpenAI client
2025-05-31 14:14:49,957 - INFO - Making API call to OpenAI
2025-05-31 14:14:52,843 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:52,844 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:52,844 - INFO - Creating OpenAI client
2025-05-31 14:14:52,851 - INFO - Making API call to OpenAI
2025-05-31 14:14:58,704 - INFO - Successfully received response from OpenAI
2025-05-31 14:14:58,704 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:14:58,704 - INFO - Creating OpenAI client
2025-05-31 14:14:58,710 - INFO - Making API call to OpenAI
2025-05-31 14:15:01,800 - INFO - Successfully received response from OpenAI
2025-05-31 14:15:01,801 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:15:01,801 - INFO - Creating OpenAI client
2025-05-31 14:15:01,808 - INFO - Making API call to OpenAI
2025-05-31 14:15:06,138 - INFO - Successfully received response from OpenAI
2025-05-31 14:15:06,138 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:15:06,138 - INFO - Creating OpenAI client
2025-05-31 14:15:06,146 - INFO - Making API call to OpenAI
2025-05-31 14:15:09,108 - INFO - Successfully received response from OpenAI
2025-05-31 14:15:09,109 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:15:09,109 - INFO - Creating OpenAI client
2025-05-31 14:15:09,115 - INFO - Making API call to OpenAI
2025-05-31 14:25:11,231 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:11,231 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:11,231 - INFO - Creating OpenAI client
2025-05-31 14:25:11,250 - INFO - Making API call to OpenAI
2025-05-31 14:25:14,546 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:14,546 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:14,546 - INFO - Creating OpenAI client
2025-05-31 14:25:14,554 - INFO - Making API call to OpenAI
2025-05-31 14:25:17,618 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:17,619 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:17,619 - INFO - Creating OpenAI client
2025-05-31 14:25:17,627 - INFO - Making API call to OpenAI
2025-05-31 14:25:22,056 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:22,057 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:22,057 - INFO - Creating OpenAI client
2025-05-31 14:25:22,064 - INFO - Making API call to OpenAI
2025-05-31 14:25:25,156 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:25,156 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:25,156 - INFO - Creating OpenAI client
2025-05-31 14:25:25,160 - INFO - Making API call to OpenAI
2025-05-31 14:25:34,789 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:34,790 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:34,790 - INFO - Creating OpenAI client
2025-05-31 14:25:34,796 - INFO - Making API call to OpenAI
2025-05-31 14:25:39,790 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:39,790 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:39,790 - INFO - Creating OpenAI client
2025-05-31 14:25:39,797 - INFO - Making API call to OpenAI
2025-05-31 14:25:42,572 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:42,572 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:42,573 - INFO - Creating OpenAI client
2025-05-31 14:25:42,580 - INFO - Making API call to OpenAI
2025-05-31 14:25:45,468 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:45,468 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:45,468 - INFO - Creating OpenAI client
2025-05-31 14:25:45,477 - INFO - Making API call to OpenAI
2025-05-31 14:25:47,961 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:47,962 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:47,962 - INFO - Creating OpenAI client
2025-05-31 14:25:47,969 - INFO - Making API call to OpenAI
2025-05-31 14:25:50,837 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:50,837 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:50,837 - INFO - Creating OpenAI client
2025-05-31 14:25:50,844 - INFO - Making API call to OpenAI
2025-05-31 14:25:52,163 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:52,163 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:52,163 - INFO - Creating OpenAI client
2025-05-31 14:25:52,171 - INFO - Making API call to OpenAI
2025-05-31 14:25:57,619 - INFO - Successfully received response from OpenAI
2025-05-31 14:25:57,619 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:25:57,619 - INFO - Creating OpenAI client
2025-05-31 14:25:57,626 - INFO - Making API call to OpenAI
2025-05-31 14:26:00,304 - INFO - Successfully received response from OpenAI
2025-05-31 14:26:00,304 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:26:00,304 - INFO - Creating OpenAI client
2025-05-31 14:26:00,310 - INFO - Making API call to OpenAI
2025-05-31 14:26:02,850 - INFO - Successfully received response from OpenAI
2025-05-31 14:26:02,850 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:26:02,850 - INFO - Creating OpenAI client
2025-05-31 14:26:02,859 - INFO - Making API call to OpenAI
2025-05-31 14:26:05,260 - INFO - Successfully received response from OpenAI
2025-05-31 14:26:05,260 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:26:05,260 - INFO - Creating OpenAI client
2025-05-31 14:26:05,266 - INFO - Making API call to OpenAI
2025-05-31 14:26:10,814 - INFO - Successfully received response from OpenAI
2025-05-31 14:28:41,857 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:28:41,857 - WARNING - Prompt too large: ~15369 tokens. Truncating to ~8000 tokens.
2025-05-31 14:28:41,857 - INFO - Creating OpenAI client
2025-05-31 14:28:41,874 - INFO - Making API call to OpenAI
2025-05-31 14:29:30,436 - INFO - Successfully received response from OpenAI
2025-05-31 14:29:30,438 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:29:30,438 - INFO - Creating OpenAI client
2025-05-31 14:29:30,442 - INFO - Making API call to OpenAI
2025-05-31 14:29:57,284 - INFO - Successfully received response from OpenAI
2025-05-31 14:29:57,284 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:29:57,284 - INFO - Creating OpenAI client
2025-05-31 14:29:57,291 - INFO - Making API call to OpenAI
2025-05-31 14:30:27,848 - INFO - Successfully received response from OpenAI
2025-05-31 14:30:27,849 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:30:27,849 - INFO - Creating OpenAI client
2025-05-31 14:30:27,854 - INFO - Making API call to OpenAI
2025-05-31 14:31:23,830 - INFO - Successfully received response from OpenAI
2025-05-31 14:31:23,836 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:31:23,836 - INFO - Creating OpenAI client
2025-05-31 14:31:23,839 - INFO - Making API call to OpenAI
2025-05-31 14:31:31,009 - INFO - Successfully received response from OpenAI
2025-05-31 14:31:31,009 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:31:31,009 - INFO - Creating OpenAI client
2025-05-31 14:31:31,013 - INFO - Making API call to OpenAI
2025-05-31 14:31:35,255 - INFO - Successfully received response from OpenAI
2025-05-31 14:31:35,255 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:31:35,255 - INFO - Creating OpenAI client
2025-05-31 14:31:35,260 - INFO - Making API call to OpenAI
2025-05-31 14:31:38,888 - INFO - Successfully received response from OpenAI
2025-05-31 14:31:38,888 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:31:38,889 - INFO - Creating OpenAI client
2025-05-31 14:31:38,895 - INFO - Making API call to OpenAI
2025-05-31 14:31:41,368 - INFO - Successfully received response from OpenAI
2025-05-31 14:31:41,369 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:31:41,369 - INFO - Creating OpenAI client
2025-05-31 14:31:41,376 - INFO - Making API call to OpenAI
2025-05-31 14:31:43,772 - INFO - Successfully received response from OpenAI
2025-05-31 14:31:43,772 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:31:43,772 - INFO - Creating OpenAI client
2025-05-31 14:31:43,779 - INFO - Making API call to OpenAI
2025-05-31 14:31:48,251 - INFO - Successfully received response from OpenAI
2025-05-31 14:31:48,251 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:31:48,251 - INFO - Creating OpenAI client
2025-05-31 14:31:48,257 - INFO - Making API call to OpenAI
2025-05-31 14:31:51,781 - INFO - Successfully received response from OpenAI
2025-05-31 14:31:51,781 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:31:51,781 - INFO - Creating OpenAI client
2025-05-31 14:31:51,788 - INFO - Making API call to OpenAI
2025-05-31 14:31:56,929 - INFO - Successfully received response from OpenAI
2025-05-31 14:31:56,929 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:31:56,929 - INFO - Creating OpenAI client
2025-05-31 14:31:56,932 - INFO - Making API call to OpenAI
2025-05-31 14:32:01,805 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:01,805 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:01,805 - INFO - Creating OpenAI client
2025-05-31 14:32:01,810 - INFO - Making API call to OpenAI
2025-05-31 14:32:04,540 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:04,540 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:04,540 - INFO - Creating OpenAI client
2025-05-31 14:32:04,547 - INFO - Making API call to OpenAI
2025-05-31 14:32:06,979 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:06,980 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:06,980 - INFO - Creating OpenAI client
2025-05-31 14:32:06,987 - INFO - Making API call to OpenAI
2025-05-31 14:32:09,305 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:09,306 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:09,306 - INFO - Creating OpenAI client
2025-05-31 14:32:09,313 - INFO - Making API call to OpenAI
2025-05-31 14:32:14,590 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:14,590 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:14,590 - INFO - Creating OpenAI client
2025-05-31 14:32:14,594 - INFO - Making API call to OpenAI
2025-05-31 14:32:25,710 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:25,710 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:25,711 - INFO - Creating OpenAI client
2025-05-31 14:32:25,716 - INFO - Making API call to OpenAI
2025-05-31 14:32:28,204 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:28,204 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:28,204 - INFO - Creating OpenAI client
2025-05-31 14:32:28,214 - INFO - Making API call to OpenAI
2025-05-31 14:32:33,030 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:33,031 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:33,031 - INFO - Creating OpenAI client
2025-05-31 14:32:33,037 - INFO - Making API call to OpenAI
2025-05-31 14:32:35,789 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:35,789 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:35,789 - INFO - Creating OpenAI client
2025-05-31 14:32:35,796 - INFO - Making API call to OpenAI
2025-05-31 14:32:40,588 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:40,588 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:40,588 - INFO - Creating OpenAI client
2025-05-31 14:32:40,594 - INFO - Making API call to OpenAI
2025-05-31 14:32:43,420 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:43,421 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:43,421 - INFO - Creating OpenAI client
2025-05-31 14:32:43,428 - INFO - Making API call to OpenAI
2025-05-31 14:32:48,214 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:48,214 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:48,214 - INFO - Creating OpenAI client
2025-05-31 14:32:48,219 - INFO - Making API call to OpenAI
2025-05-31 14:32:50,890 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:50,890 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:50,891 - INFO - Creating OpenAI client
2025-05-31 14:32:50,897 - INFO - Making API call to OpenAI
2025-05-31 14:32:54,780 - INFO - Successfully received response from OpenAI
2025-05-31 14:32:54,780 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:32:54,780 - INFO - Creating OpenAI client
2025-05-31 14:32:54,787 - INFO - Making API call to OpenAI
2025-05-31 14:33:17,765 - INFO - Successfully received response from OpenAI
2025-05-31 14:33:17,765 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:33:17,765 - INFO - Creating OpenAI client
2025-05-31 14:33:17,771 - INFO - Making API call to OpenAI
2025-05-31 14:33:21,826 - INFO - Successfully received response from OpenAI
2025-05-31 14:33:21,826 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:33:21,826 - INFO - Creating OpenAI client
2025-05-31 14:33:21,831 - INFO - Making API call to OpenAI
2025-05-31 14:33:25,541 - INFO - Successfully received response from OpenAI
2025-05-31 14:33:25,541 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:33:25,541 - INFO - Creating OpenAI client
2025-05-31 14:33:25,545 - INFO - Making API call to OpenAI
2025-05-31 14:33:29,482 - INFO - Successfully received response from OpenAI
2025-05-31 14:33:29,482 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:33:29,482 - INFO - Creating OpenAI client
2025-05-31 14:33:29,488 - INFO - Making API call to OpenAI
2025-05-31 14:33:35,289 - INFO - Successfully received response from OpenAI
2025-05-31 14:33:35,289 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:33:35,289 - INFO - Creating OpenAI client
2025-05-31 14:33:35,294 - INFO - Making API call to OpenAI
2025-05-31 14:33:42,936 - INFO - Successfully received response from OpenAI
2025-05-31 14:33:42,936 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:33:42,936 - INFO - Creating OpenAI client
2025-05-31 14:33:42,940 - INFO - Making API call to OpenAI
2025-05-31 14:33:46,220 - INFO - Successfully received response from OpenAI
2025-05-31 14:33:46,221 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:33:46,221 - INFO - Creating OpenAI client
2025-05-31 14:33:46,228 - INFO - Making API call to OpenAI
2025-05-31 14:34:07,743 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:07,744 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:07,744 - INFO - Creating OpenAI client
2025-05-31 14:34:07,750 - INFO - Making API call to OpenAI
2025-05-31 14:34:10,815 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:10,815 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:10,815 - INFO - Creating OpenAI client
2025-05-31 14:34:10,820 - INFO - Making API call to OpenAI
2025-05-31 14:34:13,811 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:13,812 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:13,812 - INFO - Creating OpenAI client
2025-05-31 14:34:13,818 - INFO - Making API call to OpenAI
2025-05-31 14:34:16,712 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:16,712 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:16,713 - INFO - Creating OpenAI client
2025-05-31 14:34:16,720 - INFO - Making API call to OpenAI
2025-05-31 14:34:19,853 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:19,853 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:19,853 - INFO - Creating OpenAI client
2025-05-31 14:34:19,861 - INFO - Making API call to OpenAI
2025-05-31 14:34:21,405 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:21,405 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:21,405 - INFO - Creating OpenAI client
2025-05-31 14:34:21,413 - INFO - Making API call to OpenAI
2025-05-31 14:34:24,254 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:24,254 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:24,254 - INFO - Creating OpenAI client
2025-05-31 14:34:24,258 - INFO - Making API call to OpenAI
2025-05-31 14:34:27,257 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:27,257 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:27,257 - INFO - Creating OpenAI client
2025-05-31 14:34:27,264 - INFO - Making API call to OpenAI
2025-05-31 14:34:30,205 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:30,205 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:30,205 - INFO - Creating OpenAI client
2025-05-31 14:34:30,224 - INFO - Making API call to OpenAI
2025-05-31 14:34:32,990 - INFO - Successfully received response from OpenAI
2025-05-31 14:34:32,991 - INFO - Starting LLM call, use_cache=True
2025-05-31 14:34:32,991 - INFO - Creating OpenAI client
2025-05-31 14:34:32,998 - INFO - Making API call to OpenAI
2025-05-31 14:34:35,910 - INFO - Successfully received response from OpenAI
2025-05-31 15:32:53,391 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:32:53,391 - WARNING - Prompt too large: ~15369 tokens. Truncating to ~8000 tokens.
2025-05-31 15:32:53,391 - INFO - Creating OpenAI client
2025-05-31 15:32:53,411 - INFO - Making API call to OpenAI
2025-05-31 15:33:32,262 - INFO - Successfully received response from OpenAI
2025-05-31 15:33:32,265 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:33:32,265 - INFO - Creating OpenAI client
2025-05-31 15:33:32,271 - INFO - Making API call to OpenAI
2025-05-31 15:34:06,138 - INFO - Successfully received response from OpenAI
2025-05-31 15:34:06,139 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:34:06,139 - INFO - Creating OpenAI client
2025-05-31 15:34:06,148 - INFO - Making API call to OpenAI
2025-05-31 15:34:34,366 - INFO - Successfully received response from OpenAI
2025-05-31 15:34:34,367 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:34:34,367 - INFO - Creating OpenAI client
2025-05-31 15:34:34,375 - INFO - Making API call to OpenAI
2025-05-31 15:34:56,979 - INFO - Successfully received response from OpenAI
2025-05-31 15:44:58,453 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:44:58,453 - WARNING - Prompt too large: ~15369 tokens. Truncating to ~8000 tokens.
2025-05-31 15:44:58,453 - INFO - Creating OpenAI client
2025-05-31 15:44:58,471 - INFO - Making API call to OpenAI
2025-05-31 15:45:32,605 - INFO - Successfully received response from OpenAI
2025-05-31 15:45:32,609 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:45:32,609 - INFO - Creating OpenAI client
2025-05-31 15:45:32,614 - INFO - Making API call to OpenAI
2025-05-31 15:46:19,151 - INFO - Successfully received response from OpenAI
2025-05-31 15:46:19,152 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:46:19,152 - INFO - Creating OpenAI client
2025-05-31 15:46:19,160 - INFO - Making API call to OpenAI
2025-05-31 15:47:10,179 - INFO - Successfully received response from OpenAI
2025-05-31 15:47:10,181 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:47:10,181 - INFO - Creating OpenAI client
2025-05-31 15:47:10,187 - INFO - Making API call to OpenAI
2025-05-31 15:48:04,798 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:48:04,798 - WARNING - Prompt too large: ~15369 tokens. Truncating to ~8000 tokens.
2025-05-31 15:48:04,798 - INFO - Creating OpenAI client
2025-05-31 15:48:04,817 - INFO - Making API call to OpenAI
2025-05-31 15:48:43,497 - INFO - Successfully received response from OpenAI
2025-05-31 15:48:43,500 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:48:43,500 - INFO - Creating OpenAI client
2025-05-31 15:48:43,505 - INFO - Making API call to OpenAI
2025-05-31 15:49:08,672 - INFO - Successfully received response from OpenAI
2025-05-31 15:49:08,673 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:49:08,673 - INFO - Creating OpenAI client
2025-05-31 15:49:08,683 - INFO - Making API call to OpenAI
2025-05-31 15:50:27,918 - INFO - Successfully received response from OpenAI
2025-05-31 15:50:27,920 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:50:27,920 - INFO - Creating OpenAI client
2025-05-31 15:50:27,929 - INFO - Making API call to OpenAI
2025-05-31 15:51:20,791 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:20,801 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:20,801 - INFO - Creating OpenAI client
2025-05-31 15:51:20,809 - INFO - Making API call to OpenAI
2025-05-31 15:51:24,646 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:24,646 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:24,646 - INFO - Creating OpenAI client
2025-05-31 15:51:24,655 - INFO - Making API call to OpenAI
2025-05-31 15:51:27,774 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:27,775 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:27,775 - INFO - Creating OpenAI client
2025-05-31 15:51:27,783 - INFO - Making API call to OpenAI
2025-05-31 15:51:31,418 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:31,419 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:31,419 - INFO - Creating OpenAI client
2025-05-31 15:51:31,429 - INFO - Making API call to OpenAI
2025-05-31 15:51:33,535 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:33,535 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:33,535 - INFO - Creating OpenAI client
2025-05-31 15:51:33,543 - INFO - Making API call to OpenAI
2025-05-31 15:51:36,035 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:36,035 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:36,035 - INFO - Creating OpenAI client
2025-05-31 15:51:36,043 - INFO - Making API call to OpenAI
2025-05-31 15:51:38,616 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:38,616 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:38,617 - INFO - Creating OpenAI client
2025-05-31 15:51:38,626 - INFO - Making API call to OpenAI
2025-05-31 15:51:42,428 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:42,428 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:42,428 - INFO - Creating OpenAI client
2025-05-31 15:51:42,437 - INFO - Making API call to OpenAI
2025-05-31 15:51:44,894 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:44,894 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:44,894 - INFO - Creating OpenAI client
2025-05-31 15:51:44,900 - INFO - Making API call to OpenAI
2025-05-31 15:51:45,879 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:45,879 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:45,880 - INFO - Creating OpenAI client
2025-05-31 15:51:45,890 - INFO - Making API call to OpenAI
2025-05-31 15:51:48,240 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:48,241 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:48,241 - INFO - Creating OpenAI client
2025-05-31 15:51:48,249 - INFO - Making API call to OpenAI
2025-05-31 15:51:50,615 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:50,616 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:50,616 - INFO - Creating OpenAI client
2025-05-31 15:51:50,625 - INFO - Making API call to OpenAI
2025-05-31 15:51:51,791 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:51,791 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:51,791 - INFO - Creating OpenAI client
2025-05-31 15:51:51,799 - INFO - Making API call to OpenAI
2025-05-31 15:51:55,811 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:55,811 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:55,812 - INFO - Creating OpenAI client
2025-05-31 15:51:55,819 - INFO - Making API call to OpenAI
2025-05-31 15:51:59,565 - INFO - Successfully received response from OpenAI
2025-05-31 15:51:59,565 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:51:59,565 - INFO - Creating OpenAI client
2025-05-31 15:51:59,574 - INFO - Making API call to OpenAI
2025-05-31 15:52:01,964 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:01,964 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:01,964 - INFO - Creating OpenAI client
2025-05-31 15:52:01,973 - INFO - Making API call to OpenAI
2025-05-31 15:52:05,629 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:05,630 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:05,630 - INFO - Creating OpenAI client
2025-05-31 15:52:05,640 - INFO - Making API call to OpenAI
2025-05-31 15:52:08,412 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:08,412 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:08,412 - INFO - Creating OpenAI client
2025-05-31 15:52:08,422 - INFO - Making API call to OpenAI
2025-05-31 15:52:11,171 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:11,171 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:11,171 - INFO - Creating OpenAI client
2025-05-31 15:52:11,182 - INFO - Making API call to OpenAI
2025-05-31 15:52:14,156 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:14,156 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:14,156 - INFO - Creating OpenAI client
2025-05-31 15:52:14,167 - INFO - Making API call to OpenAI
2025-05-31 15:52:18,793 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:18,793 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:18,793 - INFO - Creating OpenAI client
2025-05-31 15:52:18,802 - INFO - Making API call to OpenAI
2025-05-31 15:52:21,759 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:21,760 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:21,760 - INFO - Creating OpenAI client
2025-05-31 15:52:21,768 - INFO - Making API call to OpenAI
2025-05-31 15:52:25,888 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:25,888 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:25,889 - INFO - Creating OpenAI client
2025-05-31 15:52:25,898 - INFO - Making API call to OpenAI
2025-05-31 15:52:42,476 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:42,476 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:42,477 - INFO - Creating OpenAI client
2025-05-31 15:52:42,488 - INFO - Making API call to OpenAI
2025-05-31 15:52:45,976 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:45,977 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:45,977 - INFO - Creating OpenAI client
2025-05-31 15:52:45,984 - INFO - Making API call to OpenAI
2025-05-31 15:52:48,908 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:48,909 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:48,909 - INFO - Creating OpenAI client
2025-05-31 15:52:48,916 - INFO - Making API call to OpenAI
2025-05-31 15:52:53,675 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:53,676 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:53,676 - INFO - Creating OpenAI client
2025-05-31 15:52:53,684 - INFO - Making API call to OpenAI
2025-05-31 15:52:56,696 - INFO - Successfully received response from OpenAI
2025-05-31 15:52:56,696 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:52:56,696 - INFO - Creating OpenAI client
2025-05-31 15:52:56,704 - INFO - Making API call to OpenAI
2025-05-31 15:53:01,243 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:01,244 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:01,244 - INFO - Creating OpenAI client
2025-05-31 15:53:01,252 - INFO - Making API call to OpenAI
2025-05-31 15:53:06,093 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:06,094 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:06,094 - INFO - Creating OpenAI client
2025-05-31 15:53:06,102 - INFO - Making API call to OpenAI
2025-05-31 15:53:09,946 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:09,946 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:09,946 - INFO - Creating OpenAI client
2025-05-31 15:53:09,955 - INFO - Making API call to OpenAI
2025-05-31 15:53:12,736 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:12,737 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:12,737 - INFO - Creating OpenAI client
2025-05-31 15:53:12,745 - INFO - Making API call to OpenAI
2025-05-31 15:53:16,710 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:16,710 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:16,710 - INFO - Creating OpenAI client
2025-05-31 15:53:16,718 - INFO - Making API call to OpenAI
2025-05-31 15:53:19,370 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:19,370 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:19,370 - INFO - Creating OpenAI client
2025-05-31 15:53:19,379 - INFO - Making API call to OpenAI
2025-05-31 15:53:22,328 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:22,328 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:22,328 - INFO - Creating OpenAI client
2025-05-31 15:53:22,337 - INFO - Making API call to OpenAI
2025-05-31 15:53:25,063 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:25,063 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:25,063 - INFO - Creating OpenAI client
2025-05-31 15:53:25,068 - INFO - Making API call to OpenAI
2025-05-31 15:53:31,295 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:31,296 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:31,296 - INFO - Creating OpenAI client
2025-05-31 15:53:31,305 - INFO - Making API call to OpenAI
2025-05-31 15:53:34,061 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:34,061 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:34,061 - INFO - Creating OpenAI client
2025-05-31 15:53:34,070 - INFO - Making API call to OpenAI
2025-05-31 15:53:36,939 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:36,940 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:36,940 - INFO - Creating OpenAI client
2025-05-31 15:53:36,944 - INFO - Making API call to OpenAI
2025-05-31 15:53:39,565 - INFO - Successfully received response from OpenAI
2025-05-31 15:53:39,565 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:53:39,565 - INFO - Creating OpenAI client
2025-05-31 15:53:39,573 - INFO - Making API call to OpenAI
2025-05-31 15:53:42,328 - INFO - Successfully received response from OpenAI
2025-05-31 15:55:47,264 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:55:47,264 - WARNING - Prompt too large: ~15369 tokens. Truncating to ~8000 tokens.
2025-05-31 15:55:47,264 - INFO - Creating OpenAI client
2025-05-31 15:55:47,281 - INFO - Making API call to OpenAI
2025-05-31 15:56:19,575 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:56:19,575 - WARNING - Prompt too large: ~15369 tokens. Truncating to ~8000 tokens.
2025-05-31 15:56:19,575 - INFO - Creating OpenAI client
2025-05-31 15:56:19,592 - INFO - Making API call to OpenAI
2025-05-31 15:56:46,072 - INFO - Successfully received response from OpenAI
2025-05-31 15:56:46,075 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:56:46,075 - INFO - Creating OpenAI client
2025-05-31 15:56:46,084 - INFO - Making API call to OpenAI
2025-05-31 15:57:42,594 - INFO - Successfully received response from OpenAI
2025-05-31 15:57:42,595 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:57:42,595 - INFO - Creating OpenAI client
2025-05-31 15:57:42,604 - INFO - Making API call to OpenAI
2025-05-31 15:58:11,487 - INFO - Successfully received response from OpenAI
2025-05-31 15:58:11,488 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:58:11,488 - INFO - Creating OpenAI client
2025-05-31 15:58:11,497 - INFO - Making API call to OpenAI
2025-05-31 15:58:37,062 - INFO - Successfully received response from OpenAI
2025-05-31 15:58:37,075 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:58:37,075 - INFO - Creating OpenAI client
2025-05-31 15:58:37,083 - INFO - Making API call to OpenAI
2025-05-31 15:58:43,896 - INFO - Successfully received response from OpenAI
2025-05-31 15:58:43,897 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:58:43,897 - INFO - Creating OpenAI client
2025-05-31 15:58:43,903 - INFO - Making API call to OpenAI
2025-05-31 15:58:52,037 - INFO - Successfully received response from OpenAI
2025-05-31 15:58:52,037 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:58:52,037 - INFO - Creating OpenAI client
2025-05-31 15:58:52,046 - INFO - Making API call to OpenAI
2025-05-31 15:58:54,083 - INFO - Successfully received response from OpenAI
2025-05-31 15:58:54,083 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:58:54,083 - INFO - Creating OpenAI client
2025-05-31 15:58:54,093 - INFO - Making API call to OpenAI
2025-05-31 15:58:55,367 - INFO - Successfully received response from OpenAI
2025-05-31 15:58:55,367 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:58:55,367 - INFO - Creating OpenAI client
2025-05-31 15:58:55,373 - INFO - Making API call to OpenAI
2025-05-31 15:58:56,800 - INFO - Successfully received response from OpenAI
2025-05-31 15:58:56,800 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:58:56,801 - INFO - Creating OpenAI client
2025-05-31 15:58:56,810 - INFO - Making API call to OpenAI
2025-05-31 15:58:59,223 - INFO - Successfully received response from OpenAI
2025-05-31 15:58:59,224 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:58:59,224 - INFO - Creating OpenAI client
2025-05-31 15:58:59,233 - INFO - Making API call to OpenAI
2025-05-31 15:59:00,397 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:00,397 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:00,397 - INFO - Creating OpenAI client
2025-05-31 15:59:00,404 - INFO - Making API call to OpenAI
2025-05-31 15:59:03,837 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:03,838 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:03,838 - INFO - Creating OpenAI client
2025-05-31 15:59:03,847 - INFO - Making API call to OpenAI
2025-05-31 15:59:06,138 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:06,138 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:06,138 - INFO - Creating OpenAI client
2025-05-31 15:59:06,148 - INFO - Making API call to OpenAI
2025-05-31 15:59:07,599 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:07,600 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:07,600 - INFO - Creating OpenAI client
2025-05-31 15:59:07,608 - INFO - Making API call to OpenAI
2025-05-31 15:59:10,104 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:10,104 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:10,105 - INFO - Creating OpenAI client
2025-05-31 15:59:10,113 - INFO - Making API call to OpenAI
2025-05-31 15:59:11,549 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:11,549 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:11,550 - INFO - Creating OpenAI client
2025-05-31 15:59:11,557 - INFO - Making API call to OpenAI
2025-05-31 15:59:14,345 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:14,345 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:14,345 - INFO - Creating OpenAI client
2025-05-31 15:59:14,354 - INFO - Making API call to OpenAI
2025-05-31 15:59:16,867 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:16,867 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:16,867 - INFO - Creating OpenAI client
2025-05-31 15:59:16,875 - INFO - Making API call to OpenAI
2025-05-31 15:59:19,619 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:19,620 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:19,620 - INFO - Creating OpenAI client
2025-05-31 15:59:19,629 - INFO - Making API call to OpenAI
2025-05-31 15:59:22,547 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:22,547 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:22,548 - INFO - Creating OpenAI client
2025-05-31 15:59:22,557 - INFO - Making API call to OpenAI
2025-05-31 15:59:24,499 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:24,499 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:24,499 - INFO - Creating OpenAI client
2025-05-31 15:59:24,508 - INFO - Making API call to OpenAI
2025-05-31 15:59:27,458 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:27,458 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:27,458 - INFO - Creating OpenAI client
2025-05-31 15:59:27,467 - INFO - Making API call to OpenAI
2025-05-31 15:59:31,264 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:31,265 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:31,265 - INFO - Creating OpenAI client
2025-05-31 15:59:31,273 - INFO - Making API call to OpenAI
2025-05-31 15:59:35,238 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:35,238 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:35,238 - INFO - Creating OpenAI client
2025-05-31 15:59:35,243 - INFO - Making API call to OpenAI
2025-05-31 15:59:39,212 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:39,213 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:39,213 - INFO - Creating OpenAI client
2025-05-31 15:59:39,221 - INFO - Making API call to OpenAI
2025-05-31 15:59:42,959 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:42,959 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:42,960 - INFO - Creating OpenAI client
2025-05-31 15:59:42,968 - INFO - Making API call to OpenAI
2025-05-31 15:59:59,486 - INFO - Successfully received response from OpenAI
2025-05-31 15:59:59,487 - INFO - Starting LLM call, use_cache=True
2025-05-31 15:59:59,487 - INFO - Creating OpenAI client
2025-05-31 15:59:59,495 - INFO - Making API call to OpenAI
2025-05-31 16:00:11,159 - INFO - Successfully received response from OpenAI
2025-05-31 16:00:11,159 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:00:11,159 - INFO - Creating OpenAI client
2025-05-31 16:00:11,169 - INFO - Making API call to OpenAI
2025-05-31 16:00:14,830 - INFO - Successfully received response from OpenAI
2025-05-31 16:00:14,830 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:00:14,830 - INFO - Creating OpenAI client
2025-05-31 16:00:14,840 - INFO - Making API call to OpenAI
2025-05-31 16:00:18,825 - INFO - Successfully received response from OpenAI
2025-05-31 16:00:18,825 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:00:18,825 - INFO - Creating OpenAI client
2025-05-31 16:00:18,833 - INFO - Making API call to OpenAI
2025-05-31 16:00:34,283 - INFO - Successfully received response from OpenAI
2025-05-31 16:00:34,284 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:00:34,284 - INFO - Creating OpenAI client
2025-05-31 16:00:34,291 - INFO - Making API call to OpenAI
2025-05-31 16:00:41,786 - INFO - Successfully received response from OpenAI
2025-05-31 16:00:41,786 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:00:41,786 - INFO - Creating OpenAI client
2025-05-31 16:00:41,795 - INFO - Making API call to OpenAI
2025-05-31 16:00:43,819 - INFO - Successfully received response from OpenAI
2025-05-31 16:00:43,820 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:00:43,820 - INFO - Creating OpenAI client
2025-05-31 16:00:43,829 - INFO - Making API call to OpenAI
2025-05-31 16:00:49,721 - INFO - Successfully received response from OpenAI
2025-05-31 16:00:49,721 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:00:49,721 - INFO - Creating OpenAI client
2025-05-31 16:00:49,728 - INFO - Making API call to OpenAI
2025-05-31 16:00:52,896 - INFO - Successfully received response from OpenAI
2025-05-31 16:00:52,897 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:00:52,897 - INFO - Creating OpenAI client
2025-05-31 16:00:52,906 - INFO - Making API call to OpenAI
2025-05-31 16:00:57,308 - INFO - Successfully received response from OpenAI
2025-05-31 16:00:57,308 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:00:57,308 - INFO - Creating OpenAI client
2025-05-31 16:00:57,317 - INFO - Making API call to OpenAI
2025-05-31 16:01:00,210 - INFO - Successfully received response from OpenAI
2025-05-31 16:01:00,210 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:01:00,210 - INFO - Creating OpenAI client
2025-05-31 16:01:00,220 - INFO - Making API call to OpenAI
2025-05-31 16:01:05,060 - INFO - Successfully received response from OpenAI
2025-05-31 16:01:05,060 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:01:05,060 - INFO - Creating OpenAI client
2025-05-31 16:01:05,066 - INFO - Making API call to OpenAI
2025-05-31 16:01:08,069 - INFO - Successfully received response from OpenAI
2025-05-31 16:01:08,069 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:01:08,069 - INFO - Creating OpenAI client
2025-05-31 16:01:08,077 - INFO - Making API call to OpenAI
2025-05-31 16:01:11,107 - INFO - Successfully received response from OpenAI
2025-05-31 16:01:11,107 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:01:11,107 - INFO - Creating OpenAI client
2025-05-31 16:01:11,115 - INFO - Making API call to OpenAI
2025-05-31 16:01:14,022 - INFO - Successfully received response from OpenAI
2025-05-31 16:01:14,022 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:01:14,022 - INFO - Creating OpenAI client
2025-05-31 16:01:14,031 - INFO - Making API call to OpenAI
2025-05-31 16:01:16,918 - INFO - Successfully received response from OpenAI
2025-05-31 16:01:16,919 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:01:16,919 - INFO - Creating OpenAI client
2025-05-31 16:01:16,929 - INFO - Making API call to OpenAI
2025-05-31 16:01:19,619 - INFO - Successfully received response from OpenAI
2025-05-31 16:01:19,620 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:01:19,620 - INFO - Creating OpenAI client
2025-05-31 16:01:19,628 - INFO - Making API call to OpenAI
2025-05-31 16:01:22,594 - INFO - Successfully received response from OpenAI
2025-05-31 16:05:53,786 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:05:53,786 - WARNING - Prompt too large: ~15369 tokens. Truncating to ~8000 tokens.
2025-05-31 16:05:53,786 - INFO - Creating OpenAI client
2025-05-31 16:05:53,802 - INFO - Making API call to OpenAI
2025-05-31 16:06:23,249 - INFO - Successfully received response from OpenAI
2025-05-31 16:06:23,251 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:06:23,252 - INFO - Creating OpenAI client
2025-05-31 16:06:23,256 - INFO - Making API call to OpenAI
2025-05-31 16:06:49,451 - INFO - Successfully received response from OpenAI
2025-05-31 16:06:49,451 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:06:49,452 - INFO - Creating OpenAI client
2025-05-31 16:06:49,462 - INFO - Making API call to OpenAI
2025-05-31 16:07:17,146 - INFO - Successfully received response from OpenAI
2025-05-31 16:07:17,148 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:07:17,148 - INFO - Creating OpenAI client
2025-05-31 16:07:17,156 - INFO - Making API call to OpenAI
2025-05-31 16:07:53,081 - INFO - Successfully received response from OpenAI
2025-05-31 16:07:53,093 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:07:53,093 - INFO - Creating OpenAI client
2025-05-31 16:07:53,098 - INFO - Making API call to OpenAI
2025-05-31 16:08:00,429 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:00,429 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:00,429 - INFO - Creating OpenAI client
2025-05-31 16:08:00,437 - INFO - Making API call to OpenAI
2025-05-31 16:08:08,422 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:08,422 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:08,422 - INFO - Creating OpenAI client
2025-05-31 16:08:08,430 - INFO - Making API call to OpenAI
2025-05-31 16:08:12,562 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:12,562 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:12,562 - INFO - Creating OpenAI client
2025-05-31 16:08:12,569 - INFO - Making API call to OpenAI
2025-05-31 16:08:14,380 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:14,380 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:14,380 - INFO - Creating OpenAI client
2025-05-31 16:08:14,389 - INFO - Making API call to OpenAI
2025-05-31 16:08:15,572 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:15,572 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:15,572 - INFO - Creating OpenAI client
2025-05-31 16:08:15,577 - INFO - Making API call to OpenAI
2025-05-31 16:08:18,791 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:18,791 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:18,792 - INFO - Creating OpenAI client
2025-05-31 16:08:18,799 - INFO - Making API call to OpenAI
2025-05-31 16:08:19,747 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:19,747 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:19,747 - INFO - Creating OpenAI client
2025-05-31 16:08:19,757 - INFO - Making API call to OpenAI
2025-05-31 16:08:20,889 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:20,889 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:20,889 - INFO - Creating OpenAI client
2025-05-31 16:08:20,898 - INFO - Making API call to OpenAI
2025-05-31 16:08:21,768 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:21,768 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:21,769 - INFO - Creating OpenAI client
2025-05-31 16:08:21,779 - INFO - Making API call to OpenAI
2025-05-31 16:08:24,261 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:24,261 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:24,261 - INFO - Creating OpenAI client
2025-05-31 16:08:24,269 - INFO - Making API call to OpenAI
2025-05-31 16:08:26,494 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:26,495 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:26,495 - INFO - Creating OpenAI client
2025-05-31 16:08:26,504 - INFO - Making API call to OpenAI
2025-05-31 16:08:33,035 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:33,036 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:33,036 - INFO - Creating OpenAI client
2025-05-31 16:08:33,044 - INFO - Making API call to OpenAI
2025-05-31 16:08:35,364 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:35,364 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:35,364 - INFO - Creating OpenAI client
2025-05-31 16:08:35,373 - INFO - Making API call to OpenAI
2025-05-31 16:08:40,303 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:40,303 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:40,303 - INFO - Creating OpenAI client
2025-05-31 16:08:40,311 - INFO - Making API call to OpenAI
2025-05-31 16:08:44,334 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:44,335 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:44,335 - INFO - Creating OpenAI client
2025-05-31 16:08:44,344 - INFO - Making API call to OpenAI
2025-05-31 16:08:45,715 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:45,716 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:45,716 - INFO - Creating OpenAI client
2025-05-31 16:08:45,722 - INFO - Making API call to OpenAI
2025-05-31 16:08:48,870 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:48,870 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:48,870 - INFO - Creating OpenAI client
2025-05-31 16:08:48,878 - INFO - Making API call to OpenAI
2025-05-31 16:08:52,170 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:52,170 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:52,171 - INFO - Creating OpenAI client
2025-05-31 16:08:52,180 - INFO - Making API call to OpenAI
2025-05-31 16:08:55,993 - INFO - Successfully received response from OpenAI
2025-05-31 16:08:55,993 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:08:55,993 - INFO - Creating OpenAI client
2025-05-31 16:08:56,002 - INFO - Making API call to OpenAI
2025-05-31 16:09:05,182 - INFO - Successfully received response from OpenAI
2025-05-31 16:09:05,182 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:09:05,182 - INFO - Creating OpenAI client
2025-05-31 16:09:05,191 - INFO - Making API call to OpenAI
2025-05-31 16:09:07,549 - INFO - Successfully received response from OpenAI
2025-05-31 16:09:07,550 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:09:07,550 - INFO - Creating OpenAI client
2025-05-31 16:09:07,558 - INFO - Making API call to OpenAI
2025-05-31 16:09:14,398 - INFO - Successfully received response from OpenAI
2025-05-31 16:09:14,398 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:09:14,399 - INFO - Creating OpenAI client
2025-05-31 16:09:14,407 - INFO - Making API call to OpenAI
2025-05-31 16:09:40,810 - INFO - Successfully received response from OpenAI
2025-05-31 16:09:40,811 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:09:40,811 - INFO - Creating OpenAI client
2025-05-31 16:09:40,819 - INFO - Making API call to OpenAI
2025-05-31 16:09:48,525 - INFO - Successfully received response from OpenAI
2025-05-31 16:09:48,525 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:09:48,525 - INFO - Creating OpenAI client
2025-05-31 16:09:48,534 - INFO - Making API call to OpenAI
2025-05-31 16:09:52,959 - INFO - Successfully received response from OpenAI
2025-05-31 16:09:52,959 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:09:52,960 - INFO - Creating OpenAI client
2025-05-31 16:09:52,967 - INFO - Making API call to OpenAI
2025-05-31 16:19:55,176 - INFO - Successfully received response from OpenAI
2025-05-31 16:19:55,177 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:19:55,177 - INFO - Creating OpenAI client
2025-05-31 16:19:55,186 - INFO - Making API call to OpenAI
2025-05-31 16:19:58,235 - INFO - Successfully received response from OpenAI
2025-05-31 16:19:58,236 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:19:58,236 - INFO - Creating OpenAI client
2025-05-31 16:19:58,245 - INFO - Making API call to OpenAI
2025-05-31 16:20:00,992 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:00,992 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:00,992 - INFO - Creating OpenAI client
2025-05-31 16:20:00,997 - INFO - Making API call to OpenAI
2025-05-31 16:20:03,731 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:03,731 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:03,731 - INFO - Creating OpenAI client
2025-05-31 16:20:03,741 - INFO - Making API call to OpenAI
2025-05-31 16:20:22,871 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:22,872 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:22,872 - INFO - Creating OpenAI client
2025-05-31 16:20:22,880 - INFO - Making API call to OpenAI
2025-05-31 16:20:26,761 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:26,761 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:26,761 - INFO - Creating OpenAI client
2025-05-31 16:20:26,769 - INFO - Making API call to OpenAI
2025-05-31 16:20:30,654 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:30,655 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:30,655 - INFO - Creating OpenAI client
2025-05-31 16:20:30,663 - INFO - Making API call to OpenAI
2025-05-31 16:20:33,202 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:33,202 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:33,202 - INFO - Creating OpenAI client
2025-05-31 16:20:33,209 - INFO - Making API call to OpenAI
2025-05-31 16:20:37,254 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:37,254 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:37,254 - INFO - Creating OpenAI client
2025-05-31 16:20:37,262 - INFO - Making API call to OpenAI
2025-05-31 16:20:42,577 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:42,578 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:42,578 - INFO - Creating OpenAI client
2025-05-31 16:20:42,587 - INFO - Making API call to OpenAI
2025-05-31 16:20:48,244 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:48,244 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:48,244 - INFO - Creating OpenAI client
2025-05-31 16:20:48,254 - INFO - Making API call to OpenAI
2025-05-31 16:20:50,779 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:50,780 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:50,780 - INFO - Creating OpenAI client
2025-05-31 16:20:50,788 - INFO - Making API call to OpenAI
2025-05-31 16:20:53,344 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:53,344 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:53,344 - INFO - Creating OpenAI client
2025-05-31 16:20:53,353 - INFO - Making API call to OpenAI
2025-05-31 16:20:55,771 - INFO - Successfully received response from OpenAI
2025-05-31 16:20:55,772 - INFO - Starting LLM call, use_cache=True
2025-05-31 16:20:55,772 - INFO - Creating OpenAI client
2025-05-31 16:20:55,781 - INFO - Making API call to OpenAI
2025-05-31 16:21:01,484 - INFO - Successfully received response from OpenAI
